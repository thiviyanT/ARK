# Configuration for training an autoregressive model on synthetic tipr dataset


#Model architecture
model_type: dec_only #can be autoreg or dec_only. The autoreg model uses the variational autoencoder architecture while the dec_only model uses a decoder-only architecture.
d_model: 512
d_latent: 32
n_heads: 16
n_layers: 1

#Training parameters
batch_size: 256
learning_rate: 0.0001
num_epochs: 300
beta0: 0.1
beta1: 1.0

#Dataset parameters
dataset: syn-tipr
shuffle_train: false
use_padding: false
triple_order: keep #keep for keeping the dataset order as is or alpha_name for sorting categorically 
permute_triples: true  # whether to permute triples in each graph

#Generation parameters
num_diversity_samples: 10000
num_generated_test_graphs: 10000
num_generated_latent_graphs: 10000
sample_frac: 0.1
beam_width: 4

#Optimization settings
lr_scheduler: true
save_every: 50
resume_from_checkpoint: false
checkpoint_path: checkpoints/autoreg_syn_tipr.pt

#Experiment tracking
experiment_name: autoreg_vae_syn_tipr_dec_only
verify_every: 10

#Evaluation settings
use_test_for_final_eval: true  

#Compression bits estimation
compression_log_every: 10  

#ablation study settings
#set to None if no ablation is needed
# ablation_encoder: MLP 
ablation_decoder: GRU #this can be used for both the autoreg and dec_only models. In both cases the GRU decoder is used.